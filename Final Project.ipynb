{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project - Python INSY 5336\n",
    "### Prajwal Prasad\n",
    "### 1001750483"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>This block is mainly for the import of libraries necessary for this project and initializing the 3 website movie details lists.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import urllib.request, urllib.parse, urllib.error #necessary imports for this project.\n",
    "    from bs4 import BeautifulSoup\n",
    "    import ssl\n",
    "    import requests\n",
    "    import csv\n",
    "    import re\n",
    "\n",
    "    # Ignore SSL certificate errors\n",
    "    ctx = ssl.create_default_context()\n",
    "    ctx.check_hostname = False\n",
    "    ctx.verify_mode = ssl.CERT_NONE\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36'}\n",
    "\n",
    "    metacritics_list = [] #Initialize 3 empty lists to store the movie details\n",
    "    imdb_list=[]\n",
    "    rt_list=[]\n",
    "    metacritics_review_txt='' #Initialize 3 empty strings to store all the movie reviews\n",
    "    imdb_review_txt=''\n",
    "    rt_review_txt=''\n",
    "    \n",
    "except: #catch exceptions incase any occur, such as missing libraries.\n",
    "    print('Failed to import one or more libraries, please check') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>This block is mainly focused on MovieReviewDatabase database creation and connection using the sqlite connect function, and the MovieReviewTable creation using sqlite3 cursor's execute. </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table created successfully!\n"
     ]
    }
   ],
   "source": [
    "import sqlite3 as lite\n",
    "\n",
    "try:\n",
    "    con = lite.connect('MovieReviewDatabase.db') #try to create and connect to the db\n",
    "except:\n",
    "    print('Unable to connect to the database!')\n",
    "\n",
    "with con:\n",
    "    try:\n",
    "        cur = con.cursor()\n",
    "        cur.execute(\"DROP TABLE IF EXISTS MovieReviewTable\") #uncomment this incase table already exists and you want to re-test.\n",
    "        cur.execute(\"CREATE TABLE MovieReviewTable(website TEXT, rank INT, movie TEXT, genre TEXT)\") #creates a table with the given attributes and attribute type.\n",
    "        print(\"Table created successfully!\")\n",
    "    except:\n",
    "        print('Table already exists, please execute drop table first or you can skip this block!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>The fetchMetaCritics(), fetchIMDB(), fetchRT() all work very similarly and are used to fetch the top 50 movies from each of the 3 websites. First, we find out the url for the top movies of each of the website, using inspect element on chrome we can get the html tags where the information we seek are embedded. requests is used to create a HTTP request using the supplied headers, the output of that is parsed by beautifulsoup. Once we get the top 100/250 or whatever movies on the website, we narrow it down to 50 results. For each movie in that result we find the movie title using the find(), and the url of that movie which is usually in href of the movie title. \n",
    "\n",
    "With the movie url we call the getMovieDetails(). In there, we can create another request to get the details of that movie such as genre, reviews etc. As we had done before, we find the tags of the content we need (in this case genre and reviews) and parse them using beatifulsoup. The text we get is sometimes unformatted with lots of spaces and special characters that can mess up our analysis, using regular expressions we can substitute any special characters with blanks. There can be multiple reviews on a single movie so we hit 50 of them and create a single large text block of reviews which is added to the website review string and then return the genre back to the calling function.\n",
    "\n",
    "With all the fields acquired for the movie, we can append it to the main movie details list of that website. Also, create a new csv file to store the movie details and write the entire list using writerows function in one shot. I have commented the csv writing part since I am already providing the csv with loaded data, you can uncomment and try it out as well! </i>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Metacritic reviews to CSV\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#This code block is for scraping from metacritics website.\n",
    "def fetchMetaCritics():\n",
    "\n",
    "    def get_MC_MovieDetails(link): #for fetching genre and reviews of each induvidual movie\n",
    "        page = requests.get(link, headers = headers)\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        genre_lst = soup.find('div', class_ = 'genres').find_all('span') #for genre\n",
    "        if(len(genre_lst)<1):\n",
    "            raise Exception('Invalid tag name for genre')\n",
    "        genre_lst=[str(i) for i in genre_lst[2:]] #convert each tag to string object\n",
    "        genre= ','.join(genre_lst) #convert list to a single string\n",
    "        genre=genre.replace('<span>','')\n",
    "        genre=genre.replace('</span>','') #remove all the tags\n",
    "        \n",
    "        try:\n",
    "            r_link = 'https://www.metacritic.com' + soup.find_all('a', class_ = 'see_all boxed oswald')[1].get('href', None) #the users review link\n",
    "            r_page = requests.get(r_link, headers = headers)\n",
    "            r_soup = BeautifulSoup(r_page.content, \"html.parser\")\n",
    "            review_lst = r_soup.find('div', class_ = 'user_reviews').find_all('div',class_='review_body') # for reviews fetch from user reviews.\n",
    "        except:\n",
    "            r_link = 'https://www.metacritic.com' + soup.find_all('a', class_ = 'see_all boxed oswald')[0].get('href', None) #the critics review link incase there are no user reviews\n",
    "            r_page = requests.get(r_link, headers = headers)\n",
    "            r_soup = BeautifulSoup(r_page.content, \"html.parser\")\n",
    "            review_lst = r_soup.find('div', class_ = 'critic_reviews').find_all('div',class_='summary') # for reviews fetch from critics reviews.\n",
    "        \n",
    "        if(len(review_lst)<1):\n",
    "            raise Exception('Invalid tag name for review')\n",
    "            \n",
    "        for rv in review_lst[:50]:#Get 50 reviews from the review list\n",
    "            text=rv.text\n",
    "            text=text.replace('Expand','')\n",
    "            text=text.replace('Read full review','')\n",
    "            text=text.replace('\\n','')\n",
    "            text=text.strip() #remove blanks\n",
    "            text = re.sub('[^a-zA-Z0-9 \\n\\.]', '', text) #remove special characters\n",
    "            global metacritics_review_txt\n",
    "            metacritics_review_txt+= ' '+text.lower()#Keep adding each review to the large review_txt, lower case so its easier to generate frequency of words.\n",
    "        \n",
    "        return genre  #return only genre back to the calling function \n",
    "        \n",
    "            \n",
    "\n",
    "    try:\n",
    "        metacritics_url='https://www.metacritic.com/browse/movies/score/metascore/all/filtered?sort=desc' #url for top 100 movies.\n",
    "        page = requests.get(metacritics_url, headers = headers) #headers so we make a legit query and not as a bot\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        container = soup.find_all('td', class_ = 'clamp-summary-wrap')\n",
    "        if(len(container)<1):\n",
    "            raise Exception('Unable to fetch movies.Please make sure the URL is correct')\n",
    "        movies = container[:50] #here we get the top 50 movies we want\n",
    "        rank=1\n",
    "        for movie in movies: #for each movie in the top 50 store it into the metacritics list with website,rank,movie,genre\n",
    "            temp=[]\n",
    "            name = movie.find('h3').text #movie title\n",
    "            if(len(name)<1):\n",
    "                raise Exception('Tag name invalid ',movie)\n",
    "            temp.append(\"metacritics\")\n",
    "            temp.append(rank)\n",
    "            temp.append(name)\n",
    "            rank+=1\n",
    "            tag = movie.find('a', class_ = 'title')\n",
    "            link = tag.get('href', None)\n",
    "            if(len(link)<1):\n",
    "                raise Exception('link not found ',movie)\n",
    "            temp.append(get_MC_MovieDetails('https://www.metacritic.com'+link)) #fetch the movie from the href link in movie title store genre in temp\n",
    "            metacritics_list.append(temp) #append temp which contains each movie genre info to the main metacritics list\n",
    "            \n",
    "            \n",
    "        print(\"Writing Metacritic reviews to CSV\") #create a new csv file and write all the metacritics movies and reviews.\n",
    "        with open('prajwal_reviews.csv', 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerows(metacritics_list)\n",
    "        print(\"Done.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "fetchMetaCritics() #invoke the main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing IMDB reviews to CSV\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#This code block is for scraping from IMDB website.\n",
    "\n",
    "def fetchIMDB():\n",
    "    \n",
    "    def get_imdb_MovieDetails(link): #for fetching genre and reviews of each induvidual movie\n",
    "        \n",
    "        page = requests.get(link, headers = headers)\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        genre_lst = soup.find('div', class_ = 'subtext').find_all('a') #for genre\n",
    "        if(len(genre_lst)<1):\n",
    "            raise Exception('Invalid tag name for genre')\n",
    "        genre=''\n",
    "        del genre_lst[-1] #there was another anchor tag which didnt have genre info\n",
    "        for gl in genre_lst:\n",
    "            genre+= gl.text +\",\" #generate string from list, I wanted to change the way I did from previous block\n",
    "        genre=genre[:-1]\n",
    "        r_link= 'https://www.imdb.com' + soup.find('div', class_ = 'user-comments').find_all('a')[-1].get('href', None)\n",
    "        r_page = requests.get(r_link, headers = headers)\n",
    "        r_soup = BeautifulSoup(r_page.content, \"html.parser\")     \n",
    "        review_lst =r_soup.find('div', class_ = 'lister-list').find_all('div',class_='text show-more__control') # for reviews fetch from user reviews.\n",
    "        tag = r_soup.find('div',class_ = 'load-more-data')\n",
    "        ajaxurl = tag.get('data-ajaxurl',None)\n",
    "        datakey = tag.get('data-key',None)\n",
    "        second_page= f'https://www.imdb.com/{ajaxurl}?paginationKey={datakey}' #Repeat the same process to get the content from 2nd page using ajax\n",
    "        r_page = requests.get(second_page, headers = headers)\n",
    "        r_soup = BeautifulSoup(r_page.content, \"html.parser\")    \n",
    "        review_lst.extend(r_soup.find('div', class_ = 'lister-list').find_all('div',class_='text show-more__control')) # for 25 more reviews fetch from user reviews.\n",
    "\n",
    "        if(len(review_lst)<1):\n",
    "            raise Exception('Invalid tag name for review')\n",
    "        \n",
    "        for rv in review_lst[:50]:  #Get 50 reviews from the review list\n",
    "            text=rv.text\n",
    "            text=text.replace('Expand','')\n",
    "            text=text.strip() #remove blanks\n",
    "            text = re.sub('[^a-zA-Z0-9 \\n\\.]', '', text) #remove special characters\n",
    "            global imdb_review_txt\n",
    "            imdb_review_txt+= ' '+text.lower() #Keep adding each review to the large review_txt, lower case so its easier to generate frequency of words.\n",
    "            \n",
    "        return genre #return only genre back to the calling function\n",
    "\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        imdb_url='https://www.imdb.com/chart/top/?ref_=nv_mv_250'#url for top 250 movies.\n",
    "        page = requests.get(imdb_url, headers = headers) #headers so we make a legit query and not as a bot\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        container = soup.find_all('td', class_ = 'titleColumn')\n",
    "        if(len(container)<1):\n",
    "                raise Exception('Unable to fetch movies.Please make sure the URL is correct')\n",
    "        movies = container[:50] #here we get the top 50 movies we want\n",
    "        rank=1\n",
    "        for movie in movies: #for each movie in the top 50 store it into the imdb list with website,rank,movie,genre\n",
    "            temp=[]\n",
    "            name = movie.find('a').text #movie title\n",
    "            if(len(name)<1):\n",
    "                raise Exception('Tag name invalid ',movie)\n",
    "            temp.append(\"imdb\")\n",
    "            temp.append(rank)\n",
    "            temp.append(name)\n",
    "            rank+=1\n",
    "            tag = movie.find('a')\n",
    "            link = tag.get('href', None)\n",
    "            if(len(link)<1):\n",
    "                raise Exception('link not found for ',movie)\n",
    "            temp.append(get_imdb_MovieDetails('https://www.imdb.com'+link)) #fetch the movie from the href link in movie title store genre in temp\n",
    "            imdb_list.append(temp) #append temp which contains each movie genre info to the main imdb list\n",
    "            \n",
    "\n",
    "        print(\"Writing IMDB reviews to CSV\")\n",
    "        with open('prajwal_reviews.csv', 'a', newline='') as file: #Open csv file and append imdb movies without overwriting all previous data.\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerows(imdb_list)\n",
    "        print(\"Done.\")  \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "    \n",
    "fetchIMDB() #invoke the main function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Rotten Tomatoes reviews to CSV\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#This code block is for scraping from Rotten Tomatoes website.\n",
    "def fetchRT():\n",
    "    \n",
    "    def get_RT_MovieDetails(link):#for fetching genre and reviews of each induvidual movie\n",
    "        page = requests.get(link, headers = headers)\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        genre_lst = soup.find('div', class_ = 'meta-value genre').text #for genre\n",
    "        if(len(genre_lst)<1):\n",
    "            raise Exception('Invalid tag name for genre')\n",
    "        genre_lst= genre_lst.split(',')\n",
    "        genre=''\n",
    "        for g in genre_lst: #put all genres into comma sepeated string\n",
    "            g=g.strip()\n",
    "            g=g.capitalize() #the genre text was in lower case, changed to match with other websites.\n",
    "            genre+=g +','\n",
    "        genre=genre[:-1]\n",
    "        review_lst=[]\n",
    "        \n",
    "        for i in range(1,4): #Fetch reviews from pages 1-4 (This is enough to get 50 reviews)\n",
    "            try:\n",
    "                r_link = link + '/reviews?&page=' + str(i)\n",
    "                r_page = requests.get(r_link, headers = headers)\n",
    "                r_soup = BeautifulSoup(r_page.content, \"html.parser\")\n",
    "                review_lst.extend(r_soup.find('div', class_ = 'review_table').find_all('div',class_='the_review')) # for reviews fetch from all critic reviews.\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "        if(len(review_lst)<1):\n",
    "            raise Exception('Invalid tag name for review')\n",
    "        \n",
    "        for review in review_lst[:50]:#Get 50 reviews from the review list\n",
    "            text = review.text\n",
    "            text = text.strip()\n",
    "            text = re.sub('[^a-zA-Z0-9 \\n\\.]', '', text) #remove blanks and special chars\n",
    "            global rt_review_txt\n",
    "            rt_review_txt += ' '+text.lower()  #Keep adding each review to the large review_txt, lower case so its easier to generate frequency of words.\n",
    "        \n",
    "\n",
    "        return genre #return only genre back to the calling function\n",
    "        \n",
    "    \n",
    "    try:\n",
    "        rt_url='https://www.rottentomatoes.com/top/bestofrt/' #url for top 100 movies.\n",
    "        page = requests.get(rt_url, headers = headers) #headers so we make a legit query and not as a bot\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        container = soup.find('table',class_='table').find_all('a', class_ = 'unstyled articleLink')\n",
    "        if(len(container)<1):\n",
    "                raise Exception('Unable to fetch movies.Please make sure the URL is correct')\n",
    "        movies= container[:50]\n",
    "        rank=1\n",
    "        for movie in movies: #for each movie in the top 50 store it into the rt list with website,rank,movie,genre,reviews\n",
    "            temp=[]\n",
    "            temp.append(\"rotten-tomatoes\")\n",
    "            temp.append(rank)\n",
    "            name=movie.text\n",
    "            if(len(name)<1):\n",
    "                raise Exception('Tag name invalid ',movie)\n",
    "            name=name[:-6].strip()\n",
    "            temp.append(name)\n",
    "            rank+=1\n",
    "            link = movie.get('href', None)\n",
    "            if(len(link)<1):\n",
    "                raise Exception('link not found for ',movie)\n",
    "            temp.append(get_RT_MovieDetails('https://www.rottentomatoes.com'+link)) #fetch the movie from the href link in movie title store genre in temp\n",
    "            rt_list.append(temp)  #append temp which contains each movie genre info to the main rt list\n",
    "\n",
    "        print(\"Writing Rotten Tomatoes reviews to CSV\") #Open csv file and append imdb movies without overwriting all previous data.\n",
    "        with open('prajwal_reviews.csv', 'a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerows(rt_list)\n",
    "        print(\"Done.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "fetchRT() #invoke the main function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>This block is focused on writing all the data we acquired thus far into the MovieReviewTable. I have used sql parameterized insert query and cursor's executemany function, so its a lot easier to push all the movie data of each website in just 3 insert statements! </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing into table\n",
      "write successful\n"
     ]
    }
   ],
   "source": [
    "#This block is for writing all movies data into the database. Don't run it if you did not drop the table.\n",
    "try:\n",
    "    with con:\n",
    "        mc_tuple = tuple(tuple(sub) for sub in metacritics_list) #convert list of list into tuple of tuples for prepared statement, makes it lot easier to insert.\n",
    "        imdb_tuple = tuple(tuple(sub) for sub in imdb_list)\n",
    "        rt_tuple = tuple(tuple(sub) for sub in rt_list)\n",
    "        cur = con.cursor()\n",
    "        print(\"writing into table\")\n",
    "        cur.executemany(\"INSERT INTO MovieReviewTable VALUES(?, ?, ?, ?)\", mc_tuple) #inserting each of the movie details from 3 websites into the table.\n",
    "        cur.executemany(\"INSERT INTO MovieReviewTable VALUES(?, ?, ?, ?)\", imdb_tuple)\n",
    "        cur.executemany(\"INSERT INTO MovieReviewTable VALUES(?, ?, ?, ?)\", rt_tuple)\n",
    "        con.commit()\n",
    "        print(\"write successful\")\n",
    "        \n",
    "except Exception as e: #catch any exceptions that occur\n",
    "    if type(e).__name__== 'NameError': #incase previous code blocks were not executed.\n",
    "        print('Unable to establish connection to Database, Please execute code blocks in order.')\n",
    "    else:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>We create a frequency of words dictionary using the movie review text we had generated for each website. We take the reviews of each movie and after some text processing split the review text (containg all movie reviews) into words. The words are stored and counted in a dictionary, which is sorted on desencding order to get the top 50 words which occur in the text.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rotten tomatoes {'film': 504, 'one': 312, 'movie': 242, 'full': 192, 'story': 190, 'review': 173, 'spanish': 167, 'best': 161, 'films': 159, 'like': 154, 'great': 122, 'time': 114, 'ever': 100, 'good': 95, 'still': 87, 'movies': 84, 'comedy': 78, 'even': 75, 'way': 75, 'never': 75, 'much': 74, 'life': 72, 'classic': 72, 'us': 71, 'action': 69, 'see': 69, 'made': 69, 'new': 68, 'also': 68, 'well': 66, 'funny': 66, 'many': 65, 'fun': 65, 'first': 64, 'love': 64, 'characters': 63, 'performances': 63, 'family': 63, 'director': 62, 'world': 61, 'seen': 59, 'every': 58, 'superhero': 58, 'makes': 58, 'get': 56, 'work': 56, 'something': 55, 'horror': 55, 'cinema': 54, 'emotional': 53}\n",
      "imdb {'film': 6135, 'movie': 5466, 'one': 3868, 'time': 2087, 'like': 1953, 'story': 1828, 'best': 1746, 'great': 1624, 'see': 1505, 'even': 1495, 'good': 1419, 'films': 1374, 'first': 1363, 'would': 1349, 'really': 1261, 'also': 1260, 'well': 1246, 'movies': 1215, 'much': 1213, 'life': 1205, 'ever': 1196, 'characters': 1189, 'many': 1183, 'way': 1170, 'character': 1138, 'people': 1135, 'made': 1107, 'seen': 1036, 'never': 969, 'two': 963, 'get': 948, 'think': 929, 'make': 925, 'watch': 890, 'scene': 886, 'could': 881, 'dont': 879, 'love': 808, 'every': 795, 'scenes': 785, 'world': 751, 'us': 745, 'still': 743, 'say': 741, 'man': 741, 'end': 717, 'know': 683, 'action': 674, 'years': 671, 'makes': 670}\n",
      "metacritics {'movie': 1302, 'film': 1223, 'one': 642, 'best': 409, 'like': 356, 'story': 352, 'time': 324, 'great': 309, 'ever': 308, 'good': 285, 'really': 276, 'films': 235, 'movies': 228, 'see': 224, 'life': 220, 'made': 216, 'well': 198, 'character': 196, 'even': 191, 'acting': 191, 'years': 190, 'much': 182, 'way': 181, 'also': 181, 'watch': 181, 'de': 174, 'characters': 168, 'people': 168, 'dont': 163, 'would': 161, 'seen': 160, 'many': 150, 'think': 149, 'every': 144, 'masterpiece': 136, 'get': 135, 'la': 132, 'first': 128, 'never': 126, 'love': 125, 'plot': 125, 'scenes': 125, 'say': 124, 'still': 123, 'end': 121, 'scene': 121, 'make': 118, 'greatest': 116, 'work': 114, 'nothing': 114}\n"
     ]
    }
   ],
   "source": [
    "#Question 1\n",
    "\n",
    "def getTopWords(big_text): #create a dictionary counter of words using the movie reviews of each website.\n",
    "\n",
    "    if(len(big_text)<1):\n",
    "        raise Exception('Empty String.')\n",
    "\n",
    "    #stop words which add no value, and repeated often.\n",
    "    stop_words = set ([\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", '', ' ', \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"])\n",
    "\n",
    "    big_text=big_text.replace('.',' ') #replace periods with blanks in case words appear conjoint\n",
    "    words=big_text.split(' ')\n",
    "\n",
    "    dict={}\n",
    "\n",
    "    for word in words: #generate frequency of words which are not stop words\n",
    "        if word in dict:\n",
    "            dict[word]=dict[word]+1\n",
    "        elif word not in stop_words:\n",
    "            dict[word]=1\n",
    "\n",
    "    if(len(dict)<1):\n",
    "        raise Exception('The dictionary turned up empty, check if the lists are generated properly.')\n",
    "\n",
    "    top_50=sorted(dict.items(), key=lambda x: x[1], reverse=True) #get top 50 in decending order.\n",
    "    top_50=top_50[:50]\n",
    "\n",
    "    ret_dict={}\n",
    "    for item in top_50: #dictionary of word as key and fequency of occurance as value.\n",
    "        ret_dict[item[0]]=item[1]\n",
    "\n",
    "    return ret_dict\n",
    "\n",
    "\n",
    "try: #generate top 50 used words for each website.\n",
    "    imdb_wordDict = getTopWords(imdb_review_txt) #each review_txt contains all the reviews for top 50 movies of that website\n",
    "    metacritics_wordDict = getTopWords(metacritics_review_txt)\n",
    "    rt_wordDict = getTopWords(rt_review_txt)\n",
    "\n",
    "    print(\"rotten tomatoes\",rt_wordDict)\n",
    "    print(\"imdb\",imdb_wordDict)\n",
    "    print(\"metacritics\",metacritics_wordDict)\n",
    "    \n",
    "except Exception as e:\n",
    "    if type(e).__name__== 'NameError':\n",
    "        print('Please make sure to run the code blocks in order.')\n",
    "    else:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Once we get the top 50 words and their frequency  of each website, we can find out the similarity between them using the cosine similarity formula. I am using numpy to ease the calculations required. First sets of top 50 words for both inputs are created, followed by a set union to combine them and remove duplicates. For each word in the union set, we add the frequency of that word in each dictionary to the respective list (l1 for dict1 and l2 for dict2) or 0 if it doesn't occur in that dictionary. The formula is then applied on the 2 lists to get a value between 0 and 1.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity score for keywords between imdb and metacritics is:  0.953053201178429\n",
      "Similarity score for keywords between imdb and rotten tomatoes is:  0.8274495152606323\n",
      "Similarity score for keywords between rotten tomatoes and metacritics is:  0.803066869486583\n"
     ]
    }
   ],
   "source": [
    "#calculate cosine similarity of reviews between 2 movie websites using their dictionaries as input\n",
    "def similarityCalc(dict1,dict2): \n",
    "    from numpy import dot #numpy functions for cosine similarity formula.\n",
    "    from numpy.linalg import norm\n",
    "\n",
    "    l1,l2=[],[]\n",
    "\n",
    "    set1 = set(list(dict1.keys()))#convert list of keys to set so we can perform set operations needed for cos_sim.\n",
    "    set2 = set(list(dict2.keys()))\n",
    "\n",
    "    if(len(set1)<1 or len(set2)<1):\n",
    "        raise Exception('Set can not be empty.')\n",
    "\n",
    "    rvector = list(set1.union(set2))  #combine unique keys in both websites.\n",
    "\n",
    "    for w in rvector: #generate 2 lists with frequency values of each words in both websites respectively.\n",
    "        if w in set1: \n",
    "            l1.append(dict1[w]) #get the freq of word in dict 1\n",
    "        else: \n",
    "            l1.append(0) \n",
    "        if w in set2: \n",
    "            l2.append(dict2[w]) #get the freq of word in dict 2\n",
    "        else: \n",
    "            l2.append(0) \n",
    "    c = 0\n",
    "\n",
    "    cos_sim = dot(l1, l2)/(norm(l1)*norm(l2)) # the cosing similarity formula\n",
    "    if(cos_sim > 1 or cos_sim < 0):\n",
    "        raise Exception('Something went wrong.cos_similarty value can only be between 0 and 1')\n",
    "    return cos_sim\n",
    "\n",
    "\n",
    "try:\n",
    "    print(\"Similarity score for keywords between imdb and metacritics is: \",similarityCalc(imdb_wordDict,metacritics_wordDict))\n",
    "    print(\"Similarity score for keywords between imdb and rotten tomatoes is: \",similarityCalc(imdb_wordDict,rt_wordDict))\n",
    "    print(\"Similarity score for keywords between rotten tomatoes and metacritics is: \",similarityCalc(rt_wordDict,metacritics_wordDict))\n",
    "\n",
    "except Exception as e:\n",
    "    if type(e).__name__== 'NameError':\n",
    "        print('Please make sure to run the code blocks in order.')\n",
    "    else:\n",
    "        print(e)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i> We the do same process of finding similarity but with genres this time. Using the movie details list of each website, we create the genre dictionary using the generateGenreCounter(). For each movie in the top 50, we split the genres and add to a dictionary with acts as a counter. After we get the frequency of each genre in each website, we apply the same similarityCalc() to get the cosine similiarty of genres between two websites. </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity score for genres between imdb and metacritics is:  0.8857105210415298\n",
      "Similarity score for genres between imdb and rotten tomatoes is:  0.7707048953466868\n",
      "Similarity score for genres between rotten tomatoes and metacritics is:  0.6824329998331107\n"
     ]
    }
   ],
   "source": [
    "#Question 2.\n",
    "      \n",
    "def generateGenreCounter(lst): #create a frequency counter of genres in each website\n",
    "    if(len(lst)<1):\n",
    "        raise Exception('List can not be empty.')\n",
    "\n",
    "    ret={}\n",
    "\n",
    "    for item in lst:\n",
    "        if(len(item)<3):\n",
    "            raise Exception('Make sure to use the correct list with all the movie details!')\n",
    "        genres = item[3] #index 3 has the movie's genre info\n",
    "        genres = genres.split(',')\n",
    "        for genre in genres: #incase the movie has more than 1 genres split them and add seperately.\n",
    "            if genre in ret:\n",
    "                ret[genre] = ret[genre] + 1 \n",
    "            else:\n",
    "                ret[genre] = 1\n",
    "    return ret\n",
    "    \n",
    "\n",
    "       \n",
    "try:\n",
    "    imdb_genre_dict =generateGenreCounter(imdb_list) #genereate each movies genre dict using their movie details list as input\n",
    "    metacritics_genre_dict =generateGenreCounter(metacritics_list)\n",
    "    rt_genre_dict =generateGenreCounter(rt_list)\n",
    "\n",
    "    print(\"Similarity score for genres between imdb and metacritics is: \",similarityCalc(imdb_genre_dict,metacritics_genre_dict))\n",
    "    print(\"Similarity score for genres between imdb and rotten tomatoes is: \",similarityCalc(imdb_genre_dict,rt_genre_dict))\n",
    "    print(\"Similarity score for genres between rotten tomatoes and metacritics is: \",similarityCalc(rt_genre_dict,metacritics_genre_dict))\n",
    "\n",
    "except Exception as e:\n",
    "    if type(e).__name__== 'NameError':\n",
    "        print('Please make sure to run the code blocks in order.')\n",
    "    else:\n",
    "        print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
